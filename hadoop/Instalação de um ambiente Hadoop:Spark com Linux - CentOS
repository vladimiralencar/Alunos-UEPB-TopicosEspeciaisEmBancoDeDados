Instalação de um ambiente Hadoop/Spark com Linux (CentOS)

1. Baixar o ISO do CentOS (Minimal Install)
2. Fazer login com root
3. yum groupinstall "X Window System" # interface gráfica
4. yum install gnome-classic-session # Gerenciador de Janelas
5. yum install control-center gnome-terminal nautilus-open-terminal liberation-mono-fonts # interface grafica complemento
6. yum install kernel-devel # kernel do linux. 
7. yum update # atualizacao do Sistema Operacional Linux CentOS 
8. unlink /etc/systemd/system/default.target
9. ln –sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target # ativar a interface grafica
10. reboot # reinicia a máquina virtual Linux
11. yum install gcc
12. Devices -> insert Guest Addiction CD Image (Melhorar o desempenho gráfico no VirtualBox)
13. shutdown # desliga a máquina virutal

VIRTUALBOX: Clique no meu File – Export Appliance.
Será gerada uma cópia de segurança da sua máquina virtual.
VM: DataServer-v1.0.ova (Apenas SO)

14_1. yum update # atualizacao do SO + kernel para VirtualBox

14. su
15. #yum install gedit
16. #gedit /etc/sudoers # habilitar a permissao para o usuario aluno 
root  ALL=(ALL) ALL 
aluno ALL=(ALL) ALL

Isso permitirá o usuário aluno executar comandos de administrador (root)

17. sudo yum install firefox
18. Instalar o Chrome

# gedit  /etc/yum.repos.d/google-chrome.repo

[google-chrome]
name=google-chrome
baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch
enabled=1
gpgcheck=1
gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub

#  yum install google-chrome-stable

19. sudo yum install bzip2 unzip rsync wget net-tools # Aplicativos

20. instalacao do MySQL
$ sudo yum install mariadb-server mariadb
$ sudo systemctl start mariadb.service  # iniciando o servico
$ sudo systemctl enable mariadb.service  # Habilitando a inicialização no boot
$ mysqladmin -u root password 'root' # Definindo a senha do administrador
$ mysql -u root - p  # ambiente terminal do mysql
> select user, host, password from mysql.user; # Executando uma query
> 

21. instalacao do servidor ssh
$ sudo yum install openssh-server openssh-clients
$ sudo chkconfig sshd on
$ sudo service sshd start
$ sudo netstat -tulpn | grep :22
$ sudo gedit /etc/ssh/sshd_config
Port 22
AddressFamily any
ListenAdress 0.0.0.0

# Autentication:
AllowUsers aluno

$ sudo service sshd restart

22. Instalacao do Java 8
23. Instalacao do JRE
$ sudo yum install java
$ java -version
24. Instalacao do jdk
baixar o arquivo
tar -xzf jdk-...-x64.tar.gz
mv  jdk-...-x64 /usr/local
cd /opt
sudo ln -s /usr/local/jdk-...-x64 /opt/jdk # link simbolico
ls -la

sudo ln -s /usr/lib/jvm/java-1.8...._64/jre/ /opt/jre

ls -la 

25. editar o .bashrc - variaveis de ambiente
cd ~
gedit .bashrc
# Java
export JAVA_HOME=/opt/jdk
export JRE_HOME=/opt/jre
export PATH=$PATH:$JRE_HOME/bin:$JAVA_HOME/bin

source .bashrc

26. Segundo Chekpoint
File - Export Appliance
VM: dataserver-v2.0.ova (SO e Utilitarios)

27. Configuracao do Hadoop
28. Desabilitando o ipv6
sudo gedit /etc/sysctl.conf
# desabilitar ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

29. Configuracao do ssh (conexao ssh sem senha para o servidor Hadoop)
$ ssh-keygen –t rsa
$ cat ˜/.ssh/id+rsa.pub >> ˜/.ssh/autorized_keys
$ chmod 0600 ˜/.ssh/authorized_keys
$ ssh localhost

30. arquivo hosts
sudo gedit /etc/hosts
127.0.0.1	dataserver

31. Download do Hadoop
opcao 1: ir na pagina do Hadoop
opcao 2: wget http://apache.mirror.getcomm.net/hadoop/common/hadoop-...versino
$ tar -xzf hadoop-version-...
$ sudo mv hadoop-version-... /usr/local/
$ sudo ln -s  /usr/local/hadoop-version-... /opt/hadoop
$ ls -la /opt

32. configuracao do hadoop
$ cd /opt/hadoop/etc/hadoop
$ ls -la
$ gedit hadoop-env.sh

# Java
export JAVA_HOME=/opt/jdk
# pasta do hadoop
export HADOOP_PREFIX=/opt/hadoop

gedit core-site.xml
<configuration>
 <property>
   <name>fs.defaultFS</name>
 </property>
</configuration>

$ $ cd /opt/hadoop/etc/hadoop/
$ gedit hdfs-site.xml


<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
 </property>
 <property>
  <name>dfs.safemode.extension</name>
  <value>0</value>
 </property>
</configuration>

$ cp mapred-site.xml.template mapred-site.xml

<configuration>
 <property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
 </property>
</configuration>

$ gedit yarn-site.xml

<configuration>
 <property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
 </property>
</configuration>

$ cd ~
$ gedit .bashrc

# Hadoop
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADDOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

$ source .bashrc
$ hadoop version

$ hdfs namenode -format # formatar o namenode

$ start-dfs.sh  # inicializa o hadoop

$ jps # processos ativos relacionados ao hadoop

3077 Jps
2665 NameNode
2763 DataNode
2957 SecondaryNameNode

$ start-yarn.sh  # gerenciador de recursos
3144 ResourceManager
2665 NameNode
2763 DataNode
2957 SecondaryNameNode
3247 NodeManager
3311 Jps

$ no browser digitar: localhost:8088/cluster

$ hdfs dfs -mkdir /bigdata # cria o diretorio/pasta no HDFS

$ hdfs dfs -ls /  # lista os diretorios

33. acessar o portal dados.gov.br -> compras publicas -> contratos (csv)

$ hadoop fs -copyFromLocal contratos.csv /bigdata # copia o arquivo para o HDFS
$ hdfs dfs -ls /bigdata

$ hdfs dfs -cat /bigdata/contratos.csv. # exibe arquivo no HDFS

Executar 1 exemplo:
$ hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar  wordcount /bigdata/contratos.csv /output

Visualizar o resultado
hdfs dfs -cat /output/*

34. Monitorar
browser -> dataserver:50070

35. Terceiro checkpoint:
Clique no meu File – Export Appliance.
Será gerada uma cópia de segurança da sua máquina virtual.
VM: DataServer-3.0.ova (Hadoop)

36. Instalação e Configuração do Zookeeper
Download do Zookeeper
$ tar -xzf zookeeper-3.4.10.tar.gz
$ sudo mv zookeeper-3.4.10 /usr/local/
$ sudo ln -s /usr/local/zookeeper-3.4.10/ /opt/zookeeper
$ mkdir /opt/zookeeper/data
$ cd /opt/zookeeper/conf
$ cp zoo_sample.cfg zoo.cfg
$ gedit zoo.cfg

initLimit=5
syncLimit=2
dataDir=/opt/zookeeper/data

$ cd ~
$ gedit .bashrc

# Zookeeper
export ZOOKEEPER_HOME=/opt/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

$ source .bashrc

$ zkServer.sh start  # inicializa o Zookeeper

$ zkCli.sh # Zookeeper Command Line Interface (CLI)

37. Instalação e Configuração do HBase
Download
$ tar -xzf hbase-1.2.6-bin.tar.gz 
$ sudo mv hbase-1.2.6 /usr/local
$ sudo ln -s /usr/local/hbase-1.2.6/ /opt/hbase
$ cd /opt/hbase/conf
$ gedit hbase-env.sh #Editar o PATH do Java e comentar as linhas do PermSize

export JAVA_HOME=/opt/jdk

# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+
##export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m"
##export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m"

$ gedit hbase-site.xml

<configuration>
 <property>
  <name>hbase.rootdir</name>
  <value>file:///opt/hbase/hfiles</value>
 </property>

 <property>
  <name>hbase.zookeeper.property.dataDir</name>
  <value>/opt/zookeeper/data</value>
 </property>
</configuration>

$ cd ~
$ gedit .bashrc
$ source. bashrc

$ start-hbase.sh
$ hbase shell

38. Instalação e Configuração do Hive
Download
$ tar -xzf apache-hive-2.3.0-bin.tar.gz 
$ sudo mv apache-hive-2.3.0-bin /usr/local/
$ sudo ln -s /usr/local/apache-hive-2.3.0-bin/ /opt/hive

$ cd ~
$ gedit .bashrc

# Hive
export HIVE_HOME=/opt/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:.

$ source .bashrc

$ cd $HIVE_HOME/conf
$ cp hive-env.sh.template hive-env.sh
$ gedit hive-env.sh

# Set HADOOP_HOME to point to a specific hadoop install directory
# HADOOP_HOME=${bin}/../../hadoop
export HADOOP_HOME=/opt/hadoop

$ cp hive-default.xml.template hive-default.xml
$ gedit hive-default.xml


  <property>
    <name>hive.exec.local.scratchdir</name>
    <value>/tmp/hive</value>
    <description>Local scratch space for Hive jobs</description>
  </property>

  <property>
    <name>hive.downloaded.resources.dir</name>
    <value>/tmp/hive</value>
    <description>Temporary local directory for added resources in the remote file system.</description>
  </property>

  $ cd ˜
  # inicializa o schema hive
  $ schematool -initSchema -dbType derby

  # inicializa o hadoop para usar o hive (start-all.sh)
  $ start-dfs.sh 
  $ start-yarn.sh
  $ hive
  hive> show tables;
  OK
  Time taken: 1.099 seconds
  hive> 

  39. Instalação e Configuração do Pig
  Download
  $ tar -xzf pig-0.16.0.tar.gz 
  $ sudo mv pig-0.16.0 /usr/local
  $ sudo ln -s /usr/local/pig-0.16.0/ /opt/pig
  $ gedit .bashrc
  # Pig
  
  export PIG_HOME=/opt/pig
  export PATH=$PATH:$PIG_HOME/bin
  export PIG_CLASSPATH=$HADOOP_HOME/conf

  $ source .bashrc
  $ pig
  $ pig -h properties # propriedades e variaveis
  $ pig -version

 40. Instalação e Configuração do Spark
 Download
 $ tar -zxf spark-2.2.0-bin-hadoop2.7.tgz 
 $ sudo mv spark-2.2.0-bin-hadoop2.7 /usr/local/
 $ sudo ln -s /usr/local/spark-2.2.0-bin-hadoop2.7/ /opt/spark
 $ cd ~
 $ gedit .bashrc
# Spark
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin

$ source .bashrc

$ mkdir /tmp/hive
$ chmod 777 /tmp/hive

$ spark-shell

Acessar Apache Spark pelo browser em http://localhost:4040


41. Instalação e Configuração do Sqoop
Download sqoop-bin-hadoop
$ tar -xzf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz  
$ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha /usr/local/
$ sudo ln -s /usr/local/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ /opt/sqoop
$ cd ˜
$ gedit .bashrc

# Sqoop
export SQOOP_HOME=/opt/sqoop
export PATH=$PATH:$SQOOP_HOME/bin

$ source .bashrc
$ cd $SQOOP_HOME/conf
$ cp sqoop-env-template.sh sqoop-env.sh
$ gedit sqoop-env.sh
# Set Hadoop-specific environment variables here.

#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/opt/hadoop

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/opt/hadoop

#set the path to where bin/hbase is available
export HBASE_HOME=/opt/hbase

#Set the path to where bin/hive is available
export HIVE_HOME=/opt/hive

#Set the path for where zookeper config dir is
export ZOOCFGDIR=/opt/zookeeper/conf

$ sqoop version

42. Instalação e Configuração do Apache Flume
download flume
$ tar -xzf apache-flume-1.8.0-bin.tar.gz
$ sudo mv apache-flume-1.8.0-bin /usr/local/
$ sudo ln -s /usr/local/apache-flume-1.8.0-bin/ /opt/flume
$ cd ˜
$ gedit .bashrc

# Flume
export FLUME_HOME=/opt/flume
export PATH=$PATH:$FLUME_HOME/bin
export CLASSPATH=$CLASSPATH:$FLUME_HOME/lib/*

$ source .bashrc
$ cd /opt/flume/conf
$ cp flume-env.sh.template flume-env.sh
$ gedit flume-env.sh

export JAVA_HOME=/opt/jdk

$ flume-ng

43. Instalação e Configuração do Ambari
No CentOS
$ su
# cd /etc/yum.repos.d
# wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.2.0/ambari.repo
# yum install ambari-server
$ sudo ambari-server setup
$ ambari-server start

Acessar o browser – http://dataserver:8080 - usuário: admin / senha: admin

44. Quarto Checkpoint (Final):
Clique no meu File – Export Appliance.
Será gerada uma cópia de segurança da sua máquina virtual.
VM: DataServer-vFinal.ova (Completa)
